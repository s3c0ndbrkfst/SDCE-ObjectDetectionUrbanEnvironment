## Project Submission Writeup

### Project overview
This project uses data from the [Waymo Open dataset](https://waymo.com/open/) to train an object detection model and explore different training methods that are available to create an effective model. MORE

### Set up
The [README](/README.md) contains detailed steps to set up and train a model. Below is a brief summary of the process.

1. Create a local setup or in my case, use the Udacity provided workspace.
2. Download and the Waymo data.
3. Split the data appropriately into training, validation, and testing sets.
4. Perform exploratory data anlysis.
5. Train and evaluate the initial model.
6. Explore various types of data augmentations.
7. Run experiments to improve on the initial model performance.
8. Save models for each experiment and create inference videos to better understand the models performance.

### Dataset
#### Dataset analysis
Exploratory data analysis is a very important process and should be the first step taken when training a new model. Sample output images generated by the process are shown below. Given the below sample of 10 images, there are 6 taken in clear daylight, 2 including weather events, 1 taken around dusk and 1 taken at night. From this, it is likely that the entire dataset follows a similar distribution in which a majority of the images show clear daylight conditions, with a smaller majority including factors such as weather and low lighting. Knowing this, it will be key to include data augmentations that can simulate more factors to improve the models performance in all conditions and lighting instead of being focuses on perfect conditions only.

| EDA Output      |               |
| :-------------: | :-----------: |
| ![png](/ExploratoryDataAnalysis_output/output_6_0.png)          |     ![png](/ExploratoryDataAnalysis_output/output_6_1.png)     |
| ![png](/ExploratoryDataAnalysis_output/output_6_2.png)          |     ![png](/ExploratoryDataAnalysis_output/output_6_3.png)     |
| ![png](/ExploratoryDataAnalysis_output/output_6_4.png)          |     ![png](/ExploratoryDataAnalysis_output/output_6_5.png)     |
| ![png](/ExploratoryDataAnalysis_output/output_6_6.png)          |     ![png](/ExploratoryDataAnalysis_output/output_6_7.png)     |
| ![png](/ExploratoryDataAnalysis_output/output_6_8.png)          |     ![png](/ExploratoryDataAnalysis_output/output_6_9.png)     |

Additional data analysis was performed to better understand the distribution of classes throughout the images of the dataset. For a 1000 image batch of the dataset, the distribution by class follows the below chart. In this 1000 image set, the counts for each class are 17723 vehicles, 5191 pedestrians, 143 cyclists. Understanding the distribution can allow for more precise tuning of the model. For example, if there are few cyclists compared to pedestrians, it would be important to choose tuning parameters and other model factors that are able to train the model appropriately given the small number of relative instances of the cyclist class in the dataset. 


![png](/ExploratoryDataAnalysis_output/output_9_0.png)


#### Cross validation
The data supplied in the project workspace is already split appropriately with 86 files for training (~90%), 10 files for validation (10%), and 3 files for testing. This split of the data follows standard practices of 80-90% for training and 10-20% for validation. MORE

### Training
#### Reference experiment
The reference experiment ran 2500 steps, included minimal data augmentations and used the momentum optimizer with a cosine decay learning rate of 0.04.

The reference experiment had poor performance with the following [losses](/experiments/experiment0_reference/results/train.txt) summarized below.

| Step   | Loss   |
| :----: | :----: |
| 100    | 15.317 |
| 500    | 11.792 |
| 1000   | 10.362 |
| 1500   | 10.067 |
| 2000   | 9.199  |
| 2500   | 9.298  |

<img src="./experiments/experiment0_reference/results/train_graph.png"  width="60%" height="30%">
<img src="./experiments/experiment0_reference/results/stacked_graph.png"  width="60%" height="30%">

The reference experiment was unable to properly detect objects in the generated inference video.

#### Improve on the reference
Due to the poor performance of the initial training several experiments were performed to improve on this including adding data augmentations and updating the optimizer. The following table outlines the three experiments and a summary of the changes tested.

| Experiment           | Tested Changes |
| :------------------: | :-----------: |
| 1 - Augmentations    | Added data augmentations |
| 2 - Learning Rate    | Updated learning rate of base model |
| 3 - Optimizer        | Changed optimzer to Adam and implemented manual step learning rate |

The provided Explore Augmentations notebook made it easy to visualize the effects of different data augmentation methods. The outputs of my exploration can be found [here](/ExploreAugmentations_output/). I chose to use augmentations that would best simulate real variations that could be found in the available dataset images.

##### Experiment 1
Experiment 1 uses the reference model setup but adds several data augmentations, see below.

| Augmentation             | Reason |
| :----------------------: | :-----------: |
| random_horizontal_flip   | Can ... |
| random_crop_image        | Can remove some of the context of the original image |
| random_black_patches     | Can mimic obstructions in front of the target objects |
| random_adjust_brightness | Can simulate harsh light or no light conditions |
| random_adjust_saturation | Can ... |
| random_adjust_contrast   | Can ... |
| random_jpeg_quality      | Can mimic poor quality training data |
    
The results of experiment 1 show improvement to the reference model, however the final trained model still shows too high of loss and is unable to properly detect objects in the generated inference video.

##### Experiment 2
Experiment 2 uses the configuration of experiment 1, but adjusts the momentum optimzer learning rate to a lower value of 0.001. MORE

Experiment 2 showed a major improvment to the losses. MORE

The generated inference [videos](/experiments/experiment2/trained_model/) show how the improved model performs. However, there are clear instances in which the model improperly detects an object. MORE

<img src="./experiments/experiment2/trained_model/incorrect_detection.png"  width="60%" height="30%">

##### Experiment 3
Experiment 3 uses the Adam optimizer, instead of the momentum optimizer. In addition, a manual step learning rate is applied as described below.

| Step    | Learning Rate   |
| :-----: | :------: |
| Initial | 0.0002   |
| 500     | 0.0001   |
| 1000    | 0.00008  |
| 2000    | 0.00004  |

The generated inference [videos](/experiments/experiment3/trained_model/) show how this model improves performance even more. At the same instance, the Adam optimized model shows more accurate object detection.

<img src="./experiments/experiment3/trained_model/improved_detection.png"  width="60%" height="30%">

Although this model has the best performance of the three experiments, it still fails in a real world scenario. In the image above, at least one car is not detected at all. 

To increase the performance of the model even more, I think that more data augmentations and more optimizer adjustments are necessary to get a high accuracy model. Additionally, a larger dataset could help improve the general performance of the model.